# Pipeline template that defines common runtime environment variables.
variables:
  # Source Config for AML Services
  # The directory containing the scripts for training, evaluating, and registering the model
  - name: SOURCES_DIR_TRAIN
    value: aml_services
    # The path to the model training script under SOURCES_DIR_TRAIN
  - name: TRAIN_SCRIPT_PATH
    value: training/training_repairclinic.py
    # The path to the model evaluation script under SOURCES_DIR_TRAIN
  - name: EVALUATE_SCRIPT_PATH
    value: evaluate/evaluate_model.py
    # The path to the model registration script under SOURCES_DIR_TRAIN
  - name: REGISTER_SCRIPT_PATH
    value: register/register_model_repairclinic.py
    # The path to the model scoring script relative to SOURCES_DIR_TRAIN
  - name: SCORE_SCRIPT
    value: scoring/scoring_service.py
  - name: INPUT_MONITOR_SCRIPT_PATH
    value: monitor/input_monitor.py
  - name: PREDICTION_MONITOR_SCRIPT_PATH
    value: monitor/performance_monitor.py

  # Azure ML Variables
  - name: EXPERIMENT_NAME
    value: repair_clinic
  - name: DATASET_NAME
    value: full_train_dataset
  # AML Compute Cluster Config
  - name: AML_CPU_ENV_NAME
    value: dev
  - name: AML_GPU_ENV_NAME
    value: dev_gpu
#  Setup in utils vars     
#  - name: AML_ENV_TRAIN_CONDA_DEP_FILE
#    value: "../../environment_setup/conda_dependencies.yml"
  - name: AML_COMPUTE_CLUSTER_CPU_SKU
    value: STANDARD_DS2_V2
  - name: AML_COMPUTE_CLUSTER_NAME
    value: rprclnc-dev-cl
  - name: AML_CLUSTER_MIN_NODES
    value: 0
  - name: AML_CLUSTER_MAX_NODES
    value: 2
  - name: AML_CLUSTER_PRIORITY
    value: lowpriority
  # Flag to allow rebuilding the AML Environment after it was built for the first time. This enables dependency updates from conda_dependencies.yaml.
  - name: AML_REBUILD_ENVIRONMENT
    value: False
  - name: TRAIN_ON_GPU
    value: False
  - name: TRAINING_PIPELINE_NAME
    value: "repair_clinic_training"
  - name: MODEL_NAME
    value: distilbert-base-uncased
  - name: AML_ENV_VERSION
    value: 3

  #Data Variables
  # Uncomment DATASTORE_NAME if you have configured non default datastore to point to your data
  # - name: DATASTORE_NAME
  #   value: datablobstore
  - name: DATASET_VERSION
    value: latest
    # Optional. Used by a training pipeline with R on Databricks
  - name: DB_CLUSTER_ID
    value: ""


  # These are the default values set in aml_service\utils\env_variables.py. Uncomment and override if desired.
  # Set to false to disable the evaluation step in the ML pipeline and register the newly trained model unconditionally.
  - name: RUN_EVALUATION
    value: "false"
  # Set to false to register the model regardless of the outcome of the evaluation step in the ML pipeline.
  # - name: ALLOW_RUN_CANCEL
  #   value: "true"


  # The path to the batch scoring script relative to SOURCES_DIR_TRAIN
  - name: BATCHSCORE_SCRIPT_PATH
    value: scoring/parallel_batchscore.py
  - name: BATCHSCORE_COPY_SCRIPT_PATH
    value: scoring/parallel_batchscore_copyoutput.py
  # Flag to allow rebuilding the AML Environment after it was built for the first time. 
  # This enables dependency updates from the conda dependencies yaml for scoring activities.
  - name: AML_REBUILD_ENVIRONMENT_SCORING
    value: "true"

  # Datastore config for scoring
  # The storage account name and key are supplied as variables in a variable group 
  # in the Azure Pipelines library for this project. Please refer to repo docs for 
  # more details

  # Blob container where the input data for scoring can be found
  - name: SCORING_DATASTORE_INPUT_CONTAINER
    value: "input"
  # Blobname for the input data - include any applicable path in the string 
  - name: SCORING_DATASTORE_INPUT_FILENAME
    value: "scoring_input.csv"
  # Blob container where the output data for scoring can be found
  - name: SCORING_DATASTORE_OUTPUT_CONTAINER
    value: "output"
  # Blobname for the output data - include any applicable path in the string 
  - name: SCORING_DATASTORE_OUTPUT_FILENAME
    value: "scoring_output.csv"
  # Dataset name for input data for scoring
  - name: SCORING_DATASET_NAME
    value: "scoring_ds"
  # Scoring pipeline name
  - name: SCORING_PIPELINE_NAME
    value: "scoring-pipeline"


  - name: ACI_DEPLOYMENT_NAME
    value: "repair-preprod"
  - name: AKS_COMPUTE_NAME 
    value: "repairclinic-aks"
  - name: AKS_DEPLOYMENT_NAME
    value: "repairclinic-aks"
  - name: AKS_AGENT_COUNT
    value: 2
    #Values DevTest FastProd
  - name: AKS_CLUSTER_PURPOSE
    value: "DevTest"
  - name: SCORE_ENV_VARS
    value: "{'MODEL_NAME': 'distilbert-base-uncased', 
            'STORAGE_ACCOUNT_NAME': 'rprclncadls01dev',
            'SM_CONTAINER': 'inferencedata',
            'SM_BLOB': 'RC_HelpSymptomDescriptions.csv'}"
  # Variables below are used for controlling various aspects of batch scoring
  - name: USE_GPU_FOR_SCORING
    value: False

  # Conda dependancies are built in env_variables.py  
#  - name: AML_ENV_SCORE_CONDA_DEP_FILE
#    value: "../../environment_setup/conda_dependencies_scoring.yml"
  # Conda dependencies for the score copying step
#  - name: AML_ENV_SCORECOPY_CONDA_DEP_FILE
#    value: "../../environment_setup/conda_dependencies_scorecopy.yml"
    # AML Compute Cluster Config for parallel batch scoring

  - name: AML_ENV_NAME_SCORING
    value: staging
  - name: AML_ENV_NAME_SCORE_COPY
    value: prod
  - name: AML_COMPUTE_CLUSTER_CPU_SKU_SCORING
    value: STANDARD_DS2_V2
  - name: AML_COMPUTE_CLUSTER_NAME_SCORING
    value: score-cluster
  - name: AML_CLUSTER_MIN_NODES_SCORING
    value: 0
  - name: AML_CLUSTER_MAX_NODES_SCORING
    value: 4
  - name: AML_CLUSTER_PRIORITY_SCORING
    value: lowpriority
  # The name for the (docker/webapp) scoring image
  - name: IMAGE_NAME
    value: "scoringImg"

  - name: MONITOR_PIPELINE_NAME
    value: "custom_datadrift_mon"
  - name: DRIFT_THRESHOLD
    value: 0.99
  # Can be "Minute", "Hour", "Day", "Week", or "Month"
  - name: MONITOR_SCHEDULE_FREQUENCY
    value: "Week"
  - name: MONITOR_SCHEDULE_INTERVAL
    value: 2
